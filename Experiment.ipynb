{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep learning also known as deep structured learning or hierarchical learning is part of a broader family of machine learning methods based on learning data representations as opposed to taskspecific algorithms learning can be supervised partially supervised or unsupervised\n",
      "some representations are loosely based on interpretation of information processing and communication patterns in a biological nervous system such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain research attempts to create efficient systems to learn these representations from largescale unlabeled data sets\n",
      "deep learning architectures such as deep neural networks deep belief networks and recurrent neural networks have been applied to fields including computer vision speech recognition natural language processing audio recognition social network filtering machine translation and bioinformatics where they produced results comparable to and in some cases superior to human experts\n",
      "deep learning is a class of machine learning algorithms that\n",
      "use a cascade of many layers of nonlinear processing units for feature extraction and transformation each successive layer uses the output from the previous layer as input the algorithms may be supervised or unsupervised and applications include pattern analysis unsupervised and classification supervised\n"
     ]
    }
   ],
   "source": [
    "with open('corpus.txt', 'r') as f:\n",
    "    corpus = []\n",
    "    for line in f: # loops over all the lines in the corpus\n",
    "        line = line.strip() # strips off \\n \\r from the ends \n",
    "        if line: # Take only non empty lines\n",
    "            line = re.sub(r'[^a-zA-Z0-9\\s]', '', line) \n",
    "            line = line.lower()\n",
    "            line = re.sub(' +',' ', line) # Removes consecutive spaces\n",
    "            # add more pre-processing steps\n",
    "            corpus.append(line)\n",
    "print(\"\\n\".join(corpus[:5])) # Shows the first 5 lines of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert treebank tags to Wordnet tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generator(s, n, Stemming):\n",
    "    \n",
    "    if(len(s.split()) < n):\n",
    "        return \"ERROR, NUMBER OF GRAM EXCEED TEXT!\"\n",
    "\n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    token = [token for token in s.split(\" \") if token != \"\"]\n",
    "    token.insert(0,'<BOS>')\n",
    "    token.insert(-1,'<EOS>')\n",
    "    if(Stemming == True):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tagged = nltk.pos_tag(token)\n",
    "        token = []\n",
    "        for word, tag in tagged:\n",
    "            wntag = get_wordnet_pos(tag)\n",
    "            if wntag is None:# not supply tag in case of None\n",
    "                lemma = lemmatizer.lemmatize(word) \n",
    "                token.append(lemma)\n",
    "            else:\n",
    "                lemma = lemmatizer.lemmatize(word, pos = wntag) \n",
    "                token.append(lemma)\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['deep\",\n",
       " 'learning',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'deep',\n",
       " 'structured',\n",
       " 'learning',\n",
       " 'or',\n",
       " 'hierarchical']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructiong vocabulary list\n",
    "vocabulary_list = list(str(corpus).split())\n",
    "vocabulary_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 19062),\n",
       " ('of', 13563),\n",
       " ('and', 9444),\n",
       " ('in', 7130),\n",
       " ('to', 6965),\n",
       " ('a', 6740),\n",
       " ('that', 3544),\n",
       " ('development', 3292),\n",
       " ('is', 3089),\n",
       " ('for', 2891)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting word frequency\n",
    "word_counter = collections.Counter()\n",
    "for term in str(corpus).split():\n",
    "    word_counter.update({term: 1})\n",
    "word_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Idx Size: 41\n",
      "Idx2Word Size: 314168\n"
     ]
    }
   ],
   "source": [
    "vocabulary_list.append(('UNK', 1))\n",
    "Idx = range(1, len(vocabulary_list)+1)\n",
    "vocab = [t[0] for t in vocabulary_list]\n",
    "\n",
    "Word2Idx = dict(zip(vocab, Idx))\n",
    "Idx2Word = dict(zip(Idx, vocab))\n",
    "\n",
    "Word2Idx['PAD'] = 0\n",
    "Idx2Word[0] = 'PAD'\n",
    "VOCAB_SIZE = len(Word2Idx)\n",
    "print('Word2Idx Size: {}'.format(len(Word2Idx)))\n",
    "print('Idx2Word Size: {}'.format(len(Idx2Word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting Train Validation Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_validation, test = train_test_split(corpus ,test_size = 0.2,train_size = 0.8)\n",
    "train, validation = train_test_split(train_validation ,test_size = 0.25,train_size =0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<BOS> ['deep learning\",\n",
       " \"['deep learning also\",\n",
       " 'learning also known',\n",
       " 'also known as',\n",
       " 'known as deep',\n",
       " 'as deep structured',\n",
       " 'deep structured learning',\n",
       " 'structured learning or',\n",
       " 'learning or hierarchical',\n",
       " 'or hierarchical learning']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_gram = ngram_generator(str(corpus), 3 , Stemming = False)\n",
    "tri_gram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sequence_prob(in_string, n, model):\n",
    "#     in_tokens, in_lengths = preprocess_corpus(in_string)\n",
    "#     in_ids = word2idseq(in_tokens, Word2Idx)\n",
    "#     X, Y_, Y = prepare_data(in_ids, n)\n",
    "#     preds = model.predict(X)\n",
    "#     log_prob = 0.0\n",
    "#     for y_i, y in enumerate(Y):\n",
    "#         log_prob += np.log(preds[y_i, y])\n",
    "\n",
    "#     log_prob = log_prob/len(Y)\n",
    "#     return log_prob\n",
    "\n",
    "# in_strings = ['hello I am science', 'blah blah blah', 'deep learning', 'answer',\n",
    "#               'Boltzman', 'from the previous layer as input', 'ahcblheb eDHLHW SLcA']\n",
    "# for in_string in in_strings:\n",
    "#     log_prob = get_sequence_prob(in_string, 5, model)\n",
    "#     print(log_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
