{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用commom.py修改的，在句首加<BOS>,句尾加<EOS>，加了未登录词，平滑方法。\n",
    "\n",
    "ngram_generator.py 切词时句尾有“/n”,修改后去除“/n”，在句首加<BOS>,句尾加<EOS>\n",
    "加入<EOS>，<BOS>后，计算句子第一个词概率为p(word|<BOS>)，不计算1gram，都用2gram\n",
    "对n个单词的句子，加入<EOS>，<BOS>后长n+2,计算句子概率时计算n+1次2gram,算困惑度时取单词数n+1。\n",
    "\n",
    "平滑算法输出为概率矩阵bigram\n",
    "计算概率矩阵时，id = 0 为<BOS>， id= n-1 为<EOS>, 因为不会出现p(<BOS>|word) 和 p(word|<EOS>)的情况，\n",
    "所以对概率矩阵n*n, 平滑时K[i][j]范围 0<=i<=n-2, 1<=j<=n-1\n",
    "\n",
    "事先随机选择一个频率为1-3的已经记录的词，用来替换未登录词。\n",
    "\n",
    "训练集：corpus.txt前80%左右，测试集：corpus.txt后20%左右，手动分的,存成了两个txt\n",
    "困惑度大概是1000左右\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "'''\n",
    "由train_data得到test_sentence的unigram概率和bigram概率\n",
    "author:Zhu Jingwen\n",
    "计算步骤\n",
    "1. 将train_data预处理,将train_data中的单词变成原形,以列表flist输出\n",
    "2.统计flist中各单词的个数,以counter:(word,times)输出\n",
    "3.给单词表建立id,得到word2id:(word,id)\n",
    "4.得到id2word(id,word)\n",
    "5.得到二维数组,俩俩单词出现的次数\n",
    "6.得到二维数组,p(w2|w1)组成的矩阵frequence\n",
    "7.根据频率矩阵frequence计算概率矩阵bigram\n",
    "8.计算test sentence bigram\n",
    "'''\n",
    "\n",
    "def ngram_generator(s, n, k = 2):\n",
    "    if (len(s.split()) < n):\n",
    "        return \"ERROR, NUMBER OF GRAM EXCEED TEXT!\"\n",
    "\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "\n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "\n",
    "    s = s.strip() #去掉换行符\n",
    "    for i in range(k-1):  #加k-1个\"<BOS> \"和\" <EOS>\"\n",
    "        s = \"<BOS> \" + s + \" <EOS>\"\n",
    "        \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    token = [token for token in s.split(\" \") if token != \"\"]\n",
    "\n",
    "    # Stemming and Lemmatizing\n",
    "    # 将句子中每个词变为原词形式输出由原形单词组成的句子数组存储在token中\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged = pos_tag(token)\n",
    "    token = []\n",
    "    for word, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:  # not supply tag in case of None\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            token.append(lemma)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "            token.append(lemma)\n",
    "\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ngram = ngram_generator('I am a student working in the library', 3)\n",
    "# print(ngram)\n",
    "# input('press enter')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "语料库预处理,将语料库按行读取(一行为一句),将所有词还原为原型,以句子列表形式输出\n",
    "输入:\n",
    "file_name eg.'train_LM.txt' 同一目录下的txt文件\n",
    "输出:\n",
    "f_list:文件中句子以列表输出,每个句子中的词都是原型\n",
    "e.g. [['<BOS>',i', 'be', 'fine', '<EOS>'], ['<BOS>','i', 'be', 'a', 'student', '<EOS>'],...]\n",
    "输入输出举例\n",
    "输入\n",
    "i am fine\\n\n",
    "i am a student\\n\n",
    "输出\n",
    "[['<BOS>',i', 'be', 'fine', '<EOS>'], ['<BOS>','i', 'be', 'a', 'student', '<EOS>']]\n",
    "'''\n",
    "\n",
    "def f_original_shape(file_name):\n",
    "    f_list = []\n",
    "    f = open(file_name, 'r', encoding='utf-8', errors='ignore')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\": #跳过空行\n",
    "                continue\n",
    "        line = ngram_generator(line, 1)\n",
    "        f_list.append(line)\n",
    "    return f_list\n",
    "\n",
    "\n",
    "'''\n",
    "统计预处理过的语料库文件f_list中个单词出现的次数\n",
    "输入:\n",
    "预处理过的文件f_list\n",
    "输出:\n",
    "n*2的列表counter,n是词汇表总次数\n",
    "counter[n][0]是word,没有重复的word\n",
    "counter[n][1]是word出现次数\n",
    "e.g.[('be', 4), ('a', 3), ('i', 2), ('fine', 1),...]\n",
    "'''\n",
    "\n",
    "\n",
    "def generate_counter_list(flist):\n",
    "    counter = Counter()  # 词频统计\n",
    "    for sentence in flist:\n",
    "        for word in sentence:\n",
    "            counter[word] += 1  # 计算每个字的出现次数{\"word1\":times,\"word2\":times2,...}\n",
    "    counter = counter.most_common()  # 将上面的结果排序{\"word1\":top_times,\"word2\":top2_times,...}\n",
    "    return counter\n",
    "\n",
    "\n",
    "'''\n",
    "给单词增加id索引\n",
    "输入:\n",
    "(word,出现次数)\n",
    "输出:\n",
    "word2id:(word,id)\n",
    "id = 0 为\"<BOS>\"， id = n -1 为\"<EOS>\"\n",
    "'''\n",
    "\n",
    "\n",
    "def get_word2id(counter):\n",
    "    lec = len(counter)  # counter中word的个数\n",
    "    word2id = {}  # {'的': 0, '很': 1, '菜': 2, '她': 3, '好': 4, '他': 5, '香': 6}\n",
    "    id = 1\n",
    "    for i in range(lec):\n",
    "        if(counter[i][0] != \"<BOS>\" and counter[i][0] != \"<EOS>\"):\n",
    "            word2id[counter[i][0]] = id\n",
    "            id += 1\n",
    "    word2id[\"<BOS>\"] = 0\n",
    "    word2id[\"<EOS>\"] = lec - 1\n",
    "    return word2id\n",
    "\n",
    "\n",
    "'''\n",
    "将word2id反转\n",
    "输入\n",
    "word2id\n",
    "输出\n",
    "id2word:(id,word)\n",
    "'''\n",
    "\n",
    "\n",
    "def get_id2word(word2id):\n",
    "    id2word = {i: w for w, i in word2id.items()}\n",
    "    return id2word\n",
    "\n",
    "\n",
    "'''\n",
    "返回bigram组成的矩阵中俩俩词的出现个数，即频率矩阵frequence\n",
    "'''\n",
    "\n",
    "def get_frequence(word2id, flist):\n",
    "    lec = len(word2id)\n",
    "    frequence = np.zeros((lec, lec)) \n",
    "    for sentence in flist:\n",
    "        pre_word = \"<BOS>\"\n",
    "        for word in sentence:\n",
    "            if word != \"<BOS>\":\n",
    "                x = word2id[pre_word]\n",
    "                y = word2id[word]\n",
    "                frequence[x][y] += 1\n",
    "                pre_word = word\n",
    "    return frequence               \n",
    "\n",
    "\n",
    "#事先随机选择一个频率为1-3的已经记录的词，用来替换未登录词\n",
    "def select_oov(counter): \n",
    "    oov = '$$$$$$$'\n",
    "    lec = len(counter)\n",
    "    for i in range(500):\n",
    "        t = random.randint(0, lec)\n",
    "        if 1<= counter[t][1] <= 3:\n",
    "            oov = counter[t][0]\n",
    "            break\n",
    "    if oov == '$$$$$$$':\n",
    "        t = random.randint(0, lec)\n",
    "        oov = counter[t][0]\n",
    "    return oov   #oov为所选的词\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "log结果\n",
    "输入\n",
    "test_sentence,word2id,由训练数据得到的所有单词概率bigram矩阵， oov来替换未登录词\n",
    "输出\n",
    "p_bigram(test_sentence)， n 为词总数\n",
    "对n个单词的句子，加入<EOS>，<BOS>后长n+2,计算句子概率时计算n+1次2gram,算困惑度时取单词数n+1。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prob_bigram(sentence, word2id, bigram, oov):\n",
    "    \n",
    "    sentence = ngram_generator(sentence, 1)  #对句子进行处理，然后加<BOS>,<EOS>\n",
    "    \n",
    "    # s = [word2id[w] for w in sentence]  # 将句子编程id序列[wordid1,wordid2,wordid3,...]\n",
    "    p = 0.00\n",
    "    n = 0\n",
    "    for i in range(1,len(sentence)):\n",
    "        if sentence[i - 1] not in word2id: #oov来替换未登录词\n",
    "            sentence[i - 1] = oov\n",
    "        if sentence[i] not in word2id:\n",
    "            sentence[i] = oov\n",
    "        p += math.log(bigram[word2id[sentence[i - 1]], word2id[sentence[i]]], 2)\n",
    "        n += 1\n",
    "    return p, n\n",
    "\n",
    "\n",
    "'''得到整个测试集的概率'''\n",
    "def prob_bigram_T(test_filename,word2id,bigram, oov):\n",
    "    f = open(test_filename, 'r', encoding='utf-8', errors='ignore')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    n = 0\n",
    "    p=0\n",
    "    for line in lines:\n",
    "        res2, k = prob_bigram(line, word2id,bigram, oov)\n",
    "        p+=  res2\n",
    "        n += k\n",
    "    return p, n\n",
    "\n",
    "\n",
    "'''\n",
    "该函数从数据集中返回词汇数目\n",
    "Inputs:\n",
    "f: 数据集文件\n",
    "Returns:\n",
    "vocab_num: 词汇数目\n",
    "'''\n",
    "\n",
    "def get_vocab_num(f):\n",
    "    f_list = f_original_shape(f)\n",
    "    list = generate_counter_list(f_list)\n",
    "    vocab_num = len(list)\n",
    "\n",
    "    return vocab_num\n",
    "\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "vocab_num: 词汇数量，对n个单词的句子，加入<EOS>，<BOS>后长n+2,计算句子概率时计算n+1次2gram,算困惑度时对每句话取单词数n+1。\n",
    "corpus_p: 整个测试集的概率\n",
    "Returns: 交叉熵\n",
    "'''\n",
    "\n",
    "\n",
    "def cross_entropy(vocab_num, log_corpus_p):\n",
    "    cross_entropy = -(1 / vocab_num) * log_corpus_p\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "cross_entropy: 模型交叉熵\n",
    "Returns\n",
    "perplexity: 模型困惑度\n",
    "'''\n",
    "\n",
    "\n",
    "def perplexity(cross_entropy):\n",
    "    perplexity = 2 ** cross_entropy\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "平滑方法\n",
    "\"\"\"\n",
    "\n",
    "def get_unigram(counter, word2id): #输出一维数组，计算概率时不包括<BOS>,<EOS>\n",
    "    K = {x[0]:x[1] for x in counter}\n",
    "    K.pop(\"<BOS>\")\n",
    "    K.pop(\"<EOS>\")\n",
    "    unigram = [0] * len(word2id)\n",
    "    for word, x in K.items():\n",
    "        unigram[word2id[word]] = x\n",
    "    N = sum(unigram)\n",
    "    for i in range(len(word2id)):\n",
    "        unigram[i] = unigram[i] / N\n",
    "    return unigram\n",
    "\n",
    "\n",
    "def get_backoff_unigram(counter, word2id):#输出一维数组，计算概率时包括<BOS>,<EOS>\n",
    "    K = {x[0]:x[1] for x in counter}\n",
    "    unigram = [0] * len(word2id)\n",
    "    for word, x in K.items():\n",
    "        unigram[word2id[word]] = x\n",
    "    N = sum(unigram)\n",
    "    for i in range(len(word2id)):\n",
    "        unigram[i] = unigram[i] / N\n",
    "    return unigram\n",
    "\n",
    "\n",
    "'''\n",
    "未平滑\n",
    "Inputs:\n",
    "frequence: 频率矩阵\n",
    "Returns\n",
    "bigram: 概率矩阵\n",
    "'''\n",
    "\n",
    "def unsmooth(frequence): #输出二维矩阵\n",
    "    lec = len(frequence)\n",
    "    bigram = np.zeros((lec, lec))\n",
    "    for i in range(lec-1):\n",
    "        bigram[i] = frequence[i] / sum(frequence[i])\n",
    "    return bigram\n",
    "'''\n",
    "加一法\n",
    "Inputs:\n",
    "frequence: 频率矩阵\n",
    "Returns\n",
    "bigram: 概率矩阵\n",
    "'''\n",
    "\n",
    "def add_one(frequence):\n",
    "    lec = len(frequence)\n",
    "    bigram = np.zeros((lec, lec))\n",
    "    for i in range(lec-1):\n",
    "        N = sum(frequence[i]) + lec - 1\n",
    "        bigram[i][1:] = (frequence[i][1:] + 1) / N            \n",
    "    return bigram\n",
    "\n",
    "'''\n",
    "good_turing\n",
    "Inputs:\n",
    "frequence: 频率矩阵\n",
    "Returns\n",
    "bigram: 概率矩阵\n",
    "'''\n",
    "\n",
    "def good_turing(frequence):\n",
    "    lec = len(frequence)\n",
    "    bigram = np.zeros((lec, lec))\n",
    "    for i in range(lec-1):\n",
    "        Rn_count = {}\n",
    "        for x in frequence[i]:\n",
    "            Rn_count[x] = Rn_count.get(x, 0) + 1\n",
    "        Rn2Rxing = {}\n",
    "        r_max = max(Rn_count.keys())\n",
    "        r = 0\n",
    "        while(r<r_max):\n",
    "            if r in Rn_count:\n",
    "                rj = r + 1\n",
    "                while(rj not in Rn_count):\n",
    "                    rj += 1\n",
    "                Rn2Rxing[r] = Rn_count[rj] * rj / Rn_count[r]\n",
    "                r = rj\n",
    "            else:\n",
    "                r += 1\n",
    "        Rn2Rxing[r_max] = r_max\n",
    "        for j in range(1, lec):\n",
    "            rn = frequence[i][j]\n",
    "            bigram[i][j] = Rn2Rxing[rn]\n",
    "        bigram[i] = bigram[i] / sum(bigram[i])        \n",
    "    return bigram\n",
    "\n",
    "'''\n",
    "Katz\n",
    "Inputs:\n",
    "frequence: 频率矩阵，backoff_unigram:一维数组unigram\n",
    "Returns\n",
    "bigram: 概率矩阵\n",
    "'''\n",
    "\n",
    "def Katz(frequence, backoff_unigram):\n",
    "    p_good_turing = good_turing(frequence)\n",
    "    lec = len(frequence)\n",
    "    bigram = np.zeros((lec, lec))\n",
    "    for i in range(lec-1):\n",
    "        beta = 0\n",
    "        Na = 0\n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0:\n",
    "                beta += p_good_turing[i][j]\n",
    "                Na += backoff_unigram[j]\n",
    "        alpha = (1 - beta) / (1 - Na)                    \n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0:\n",
    "                bigram[i][j] = p_good_turing[i][j]               \n",
    "            else:\n",
    "                bigram[i][j] = alpha * backoff_unigram[j]\n",
    "        bigram[i] = bigram[i] / sum(bigram[i])\n",
    "    return bigram\n",
    "\n",
    "'''\n",
    "绝对减值\n",
    "Inputs:\n",
    "frequence: 频率矩阵，backoff_unigram:一维数组unigram\n",
    "Returns\n",
    "bigram: 概率矩阵\n",
    "'''\n",
    "\n",
    "def absolute_discouting(frequence, backoff_unigram):\n",
    "    b = 0.75\n",
    "    lec = len(frequence)\n",
    "    bigram = np.zeros((lec, lec))\n",
    "    for i in range(lec-1):\n",
    "        N = sum(frequence[i])\n",
    "        num = 0\n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0:\n",
    "                num += 1\n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0:\n",
    "                bigram[i][j] = (frequence[i][j] - b) / N + b / N * num * backoff_unigram[j]\n",
    "            else:\n",
    "                bigram[i][j] = b / N * num * backoff_unigram[j]\n",
    "        bigram[i] = bigram[i] / sum(bigram[i])\n",
    "    return bigram\n",
    "        \n",
    "'''\n",
    "线性减值\n",
    "Inputs:\n",
    "frequence: 频率矩阵，backoff_unigram:一维数组unigram\n",
    "Returns\n",
    "bigram: 概率矩阵\n",
    "'''\n",
    "    \n",
    "def linear_discouting(frequence):\n",
    "    lec = len(frequence)\n",
    "    bigram = np.zeros((lec, lec))\n",
    "    for i in range(lec-1):\n",
    "        r1_count = 0\n",
    "        num = 0\n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0:\n",
    "                num += 1\n",
    "            if frequence[i][j] == 1:\n",
    "                r1_count += 1\n",
    "        N = sum(frequence[i])\n",
    "        alpha = r1_count / N\n",
    "        if(alpha == 0 or alpha > 0.75):\n",
    "            alpha = 0.25\n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0: \n",
    "                bigram[i][j] = frequence[i][j] * (1 - alpha) / N\n",
    "            else:\n",
    "                bigram[i][j] = alpha / (lec - 1 - num)\n",
    "    return bigram\n",
    "\n",
    "'''\n",
    "Kneser_Ney\n",
    "Inputs:\n",
    "frequence: 频率矩阵，counter，word2id\n",
    "Returns\n",
    "bigram: 概率矩阵\n",
    "'''\n",
    "    \n",
    "def Kneser_Ney(frequence, counter, word2id):\n",
    "    backoff_unigram = get_backoff_unigram(counter, word2id)\n",
    "    K = {x[0]:x[1] for x in counter}\n",
    "    count = [0] * len(word2id)\n",
    "    for word, x in K.items():\n",
    "        count[word2id[word]] = x    \n",
    "    \n",
    "    b = 0.75\n",
    "    lec = len(frequence)\n",
    "    bigram = np.zeros((lec, lec))\n",
    "    kn2 = 0\n",
    "    for i in range(lec-1):\n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0:\n",
    "                kn2 += 1\n",
    "    \n",
    "    for i in range(lec-1):\n",
    "        N = sum(frequence[i])\n",
    "        num = 0\n",
    "        for j in range(1, lec):\n",
    "            if frequence[i][j] != 0:\n",
    "                num += 1\n",
    "        for j in range(1, lec):\n",
    "            kn = count[j]\n",
    "            pkn = kn / kn2\n",
    "            if frequence[i][j] != 0:\n",
    "                bigram[i][j] = (frequence[i][j] - b) / N + b / N * num * pkn\n",
    "            else:\n",
    "                bigram[i][j] = b / N * num * pkn\n",
    "        bigram[i] = bigram[i] / sum(bigram[i])\n",
    "    return bigram\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "处理训练数据\n",
    "Inputs:\n",
    "frequence: 训练文本.txt\n",
    "Returns: frequence, word2id, counter, oov\n",
    "'''\n",
    "\n",
    "def get_train(train_file):\n",
    "    flist = f_original_shape(train_file)\n",
    "    counter = generate_counter_list(flist)\n",
    "    word2id = get_word2id(counter)\n",
    "    oov = select_oov(counter)\n",
    "    frequence = get_frequence(word2id, flist)\n",
    "    return frequence, word2id, counter, oov\n",
    "\n",
    "\n",
    "'''\n",
    "选择平滑方法计算概率矩阵\n",
    "Inputs:smooth：平滑方法, frequence, word2id, counter\n",
    "Returns: bigram:概率矩阵\n",
    "'''\n",
    "def get_bigram(smooth, frequence, word2id, counter):\n",
    "    if smooth == add_one or smooth == good_turing or smooth ==linear_discouting:\n",
    "        bigram = smooth(frequence)\n",
    "    elif smooth == Katz or smooth == absolute_discouting:\n",
    "        backoff_unigram = get_backoff_unigram(counter, word2id)\n",
    "        bigram = smooth(frequence, backoff_unigram)\n",
    "    elif smooth == Kneser_Ney:\n",
    "        bigram = smooth(frequence, counter, word2id)\n",
    "    else:\n",
    "        return print(\"false\")\n",
    "    return bigram\n",
    "\n",
    "'''\n",
    "计算测试集\n",
    "Inputs:smooth：test_file：测试集, bigram, word2id, oov\n",
    "Returns: n 单词数, res2 log概率, pp 困惑熵\n",
    "'''\n",
    "\n",
    "def get_test(test_file, bigram, word2id, oov):\n",
    "    res2, n = prob_bigram_T(test_file,word2id,bigram, oov)\n",
    "    cross_entropy_res=cross_entropy(n,res2)\n",
    "    pp=perplexity(cross_entropy_res)\n",
    "    return n, res2, pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "测试\n",
    "'''\n",
    "\n",
    "train_file = \"train01.txt\"\n",
    "test_txt = 'test01.txt'\n",
    "frequence, word2id, counter, oov = get_train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数： 32733\n",
      "加一法 log概率: -414248.117709618\n",
      "加一法 困惑熵: 6451.26643392563\n"
     ]
    }
   ],
   "source": [
    "bigram1 = get_bigram(add_one, frequence, word2id, counter)#加一法\n",
    "n, res2, pp = get_test(test_txt, bigram1, word2id, oov)\n",
    "print(\"单词数：\", n)\n",
    "print(\"加一法 log概率:\",res2)\n",
    "print(\"加一法 困惑熵:\",pp)\n",
    "\n",
    "\"\"\"\n",
    "单词数： 32733\n",
    "加一法 log概率: -414248.117709618\n",
    "加一法 困惑熵: 6451.26643392563\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数： 32733\n",
      "good_turing log概率: -368039.33247384644\n",
      "good_turing 困惑熵: 2424.8480434766625\n"
     ]
    }
   ],
   "source": [
    "bigram2 = get_bigram(good_turing, frequence, word2id, counter) #good_turing\n",
    "n, res2, pp = get_test(test_txt, bigram2, word2id, oov)\n",
    "print(\"单词数：\", n)\n",
    "print(\"good_turing log概率:\",res2)\n",
    "print(\"good_turing 困惑熵:\",pp)\n",
    "\"\"\"\n",
    "单词数： 32733\n",
    "good_turing log概率: -368039.33247384644\n",
    "good_turing 困惑熵: 2424.8480434766625\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数： 32733\n",
      "Katz log概率: -328911.47149224614\n",
      "Katz 困惑熵: 1058.8733198893017\n"
     ]
    }
   ],
   "source": [
    "bigram3 = get_bigram(Katz, frequence, word2id, counter)#Katz\n",
    "test_txt = 'test01.txt' \n",
    "n, res2, pp = get_test(test_txt, bigram3, word2id, oov)\n",
    "print(\"单词数：\", n)\n",
    "print(\"Katz log概率:\",res2)\n",
    "print(\"Katz 困惑熵:\",pp)\n",
    "\"\"\"\n",
    "单词数： 32733\n",
    "Katz log概率: -328911.47149224614\n",
    "Katz 困惑熵: 1058.8733198893017\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数： 32733\n",
      "绝对减值 log概率: -323992.9773572847\n",
      "绝对减值 困惑熵: 954.1374228207516\n"
     ]
    }
   ],
   "source": [
    "bigram4 = get_bigram(absolute_discouting, frequence, word2id, counter)#绝对减值\n",
    "test_txt = 'test01.txt'\n",
    "n, res2, pp = get_test(test_txt, bigram4, word2id, oov)\n",
    "print(\"单词数：\", n)\n",
    "print(\"绝对减值 log概率:\",res2)\n",
    "print(\"绝对减值 困惑熵:\",pp)\n",
    "\"\"\"\n",
    "单词数： 32733\n",
    "绝对减值 log概率: -323992.9773572847\n",
    "绝对减值 困惑熵: 954.1374228207516\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性减值 log概率: -368769.0588474455\n",
      "线性减值 困惑熵: 2462.609089588613\n"
     ]
    }
   ],
   "source": [
    "bigram5 = get_bigram(linear_discouting, frequence, word2id, counter)#线性减值\n",
    "test_txt = 'test01.txt'\n",
    "n, res2, pp = get_test(test_txt, bigram5, word2id, oov)\n",
    "print(\"单词数：\", n)\n",
    "print(\"线性减值 log概率:\",res2)\n",
    "print(\"线性减值 困惑熵:\",pp)\n",
    "\"\"\"\n",
    "单词数： 32733\n",
    "线性减值 log概率: -368769.0588474455\n",
    "线性减值 困惑熵: 2462.609089588613\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词数： 32733\n",
      "Kneser_Ney log概率: -323318.7272959322\n",
      "Kneser_Ney 困惑熵: 940.6112515031547\n"
     ]
    }
   ],
   "source": [
    "bigram6 = get_bigram(Kneser_Ney, frequence, word2id, counter)#Kneser_Ney\n",
    "test_txt = 'test01.txt'\n",
    "n, res2, pp = get_test(test_txt, bigram6, word2id, oov)\n",
    "print(\"单词数：\", n)\n",
    "print(\"Kneser_Ney log概率:\",res2)\n",
    "print(\"Kneser_Ney 困惑熵:\",pp)\n",
    "\"\"\"\n",
    "单词数： 32733\n",
    "Kneser_Ney log概率: -323318.7272959322\n",
    "Kneser_Ney 困惑熵: 940.6112515031547\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
