{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "写了课件里的几个平滑算法：加一法，good-turing,Katz回退法，绝对减值，线性减值\n",
    "加一法，good-turing,绝对减值，线性减值适用2gram。3gram时容易出现历史信息即前两个词的序列不存在概率为零的情况\n",
    "Katz回退法可以3gram，回退用递归写的，3gram时算的比较慢。\n",
    "\n",
    "没有考虑遇到未登录词的情况。\n",
    "未登录词写成UNK后怎么加入训练集怎么计算概率？\n",
    "\n",
    "训练集train1.txt是截的corpus.txt的一小部分.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#处理句子，加n-1个\"<BOS> \"和\" <EOS>\"\n",
    "def chuli(s, n): \n",
    "\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    s = s.strip() #去掉换行符\n",
    "    for i in range(n-1):  #加n-1个\"<BOS> \"和\" <EOS>\"\n",
    "        s = \"<BOS> \" + s + \" <EOS>\"\n",
    "    return s\n",
    "\n",
    "#将句子切成n元词组\n",
    "def ngram_generator(s, n): \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    token = [token for token in s.split(\" \") if token != \"\"]\n",
    "\n",
    "    # Stemming and Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged = pos_tag(token)\n",
    "    token = []\n",
    "    for word, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:  # not supply tag in case of None\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            token.append(lemma)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "            token.append(lemma)\n",
    "\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    return ngrams\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#读取文档，将n元词组存入字典\n",
    "def diction(file, n, k): #参数k表示为每个句子增加（k-1)个<BOS>,<EOS>\n",
    "    with open(file, 'r') as f:\n",
    "        diction = Counter()\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            line = chuli(line, k)\n",
    "            sentence = ngram_generator(line, n)\n",
    "            diction += Counter(sentence)\n",
    "    return diction\n",
    "\n",
    "#生成词组的频率数组fre，fre[i]储存i元词组的频率字典, 0<=i<=n\n",
    "def frequency(file,n):\n",
    "    fre = []\n",
    "    for i in range(1,n+1):\n",
    "        word = diction(file, i, n)\n",
    "        fre.append(word)\n",
    "    num = sum(fre[0].values())\n",
    "    fre = [{(\"num\",):num}] + fre       \n",
    "    return fre\n",
    "\n",
    "#对一个词word,计算未平滑的概率\n",
    "def unsmooth(word, n, frequency):\n",
    "    count = frequency[n].get(word, 0)\n",
    "    pre_word = word[:-1] if len(word[:-1]) >= 1 else (\"num\",)\n",
    "    p = count / frequency[n-1][pre_word]\n",
    "    return p\n",
    "        \n",
    "#对一个词word,计算加一法平滑的概率\n",
    "def jiayifa(word, n, frequency):\n",
    "    count = frequency[n].get(word, 0)\n",
    "    N = len(frequency[1]) - (2 if n > 1 else 0)\n",
    "    pre_word = word[:-1] if len(word[:-1]) >= 1 else (\"num\",)        \n",
    "    p = (count + 1) / (frequency[n-1][pre_word] + N)\n",
    "    return p\n",
    "\n",
    "#对一个词word,计算good-turing平滑的概率\n",
    "def good_turing(word, n, frequency):\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency)       \n",
    "        \n",
    "    pre_word = word[:-1] if len(word[:-1]) >= 1 else (\"num\",)\n",
    "    num = frequency[n-1][pre_word]\n",
    "    R_n = {}\n",
    "    for key, value in frequency[n].items():\n",
    "        if key[:-1] == word[:-1]:\n",
    "            R_n[value] = R_n.get(value, 0) + 1                     \n",
    "    R_n[0] = len(frequency[1]) - sum(R_n.values())\n",
    "    RtoRxing = {}\n",
    "    r_max = max(R_n.keys())\n",
    "    r = 0\n",
    "    while(r<r_max):\n",
    "        if r in R_n:\n",
    "            rj = r + 1\n",
    "            while(rj not in R_n):\n",
    "                rj += 1\n",
    "            RtoRxing[r] = R_n[rj] * rj / R_n[r]\n",
    "            r = rj\n",
    "        else:\n",
    "            r += 1\n",
    "    RtoRxing[r_max] = r_max\n",
    "    N = 0\n",
    "    for key in R_n:\n",
    "        N += R_n[key] * RtoRxing[key]\n",
    "    rk = frequency[n].get(word, 0)\n",
    "    p = RtoRxing[rk] / N\n",
    "    return p\n",
    "\n",
    "#对一个词word,计算Katz平滑的概率， Katz里面含递归，算的慢。\n",
    "#Katz是回退法可以算3gram，可以处理3gram中前两个词排列未出现的情况。\n",
    "def Katz(word, n, frequency):\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency) \n",
    "    if word in frequency[n]:\n",
    "        return good_turing(word, n, frequency)\n",
    "    else:\n",
    "        beta = 0\n",
    "        Na = 0\n",
    "        wordin = []\n",
    "        for word2 in frequency[n]:\n",
    "            if word2[:-1] == word[:-1]:\n",
    "                wordin.append(word2[-1])\n",
    "                beta += good_turing(word2, n, frequency)\n",
    "        for word3 in frequency[1]:\n",
    "            if word3 not in wordin:\n",
    "                pre_word = word[1:-1] + word3\n",
    "                Na += Katz(pre_word, n - 1, frequency)\n",
    "                              \n",
    "        alpha = (1- beta) / Na\n",
    "        return alpha * Katz(word[1:], n - 1, frequency)\n",
    "\n",
    "#对一个词word,计算绝对减值法平滑的概率，b为自由参数0<b<=1\n",
    "def absolute_discouting(word, n, frequency, b=0.4): #b为自由参数0<b<=1\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency)       \n",
    "    \n",
    "    N = frequency[n-1][word[:-1]]\n",
    "    num = 0\n",
    "    for key in frequency[n].keys():\n",
    "        if key[:-1] == word[:-1]:\n",
    "            num += 1\n",
    "    if word in frequency[n]:\n",
    "        p = (frequency[n][word] - b) / N\n",
    "    else:\n",
    "        p = b * num / (len(frequency[1]) - num) / N        \n",
    "    return p\n",
    "\n",
    "#对一个词word,计算线性减值法平滑的概率\n",
    "def linear_discouting(word, n, frequency):\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency)       \n",
    "    \n",
    "    N = frequency[n-1][word[:-1]]\n",
    "    num = 0\n",
    "    n1 = 0\n",
    "    for key in frequency[n].keys():\n",
    "        if key[:-1] == word[:-1]:\n",
    "            num += 1\n",
    "            if frequency[n][key] == 1:\n",
    "                n1 += 1\n",
    "    alpha = n1 / N\n",
    "    if word in frequency[n]:\n",
    "        p = frequency[n][word] * (1 - alpha) / N\n",
    "    else:\n",
    "        p = alpha / (len(frequency[1]) - num)       \n",
    "    return p\n",
    "    \n",
    "#计算句子s的概率    \n",
    "def calP123(s, n,frequency, smooth):\n",
    "    s = chuli(s, n)\n",
    "    t = ngram_generator(s, n)\n",
    "    P = 1\n",
    "    sentence = []\n",
    "    for word in t:\n",
    "        pk = smooth(word, n, frequency)\n",
    "        P *= pk\n",
    "        sentence.append([word,pk])\n",
    "    print(str(smooth))\n",
    "    print(sentence)\n",
    "    print(s, \"概率是\", P)\n",
    "    return P    \n",
    "        \n",
    "#计算句子s的概率， 加了try,词组在smooth频率数组中不存在时，概率为0          \n",
    "def calP(s, n,frequency, smooth):\n",
    "    s = chuli(s, n)\n",
    "    t = ngram_generator(s, n)\n",
    "    P = 1\n",
    "    sentence = []\n",
    "    for word in t:\n",
    "        try:\n",
    "            pk = smooth(word, n, frequency)\n",
    "        except:\n",
    "            pk = 0\n",
    "        P *= pk\n",
    "        sentence.append([word,pk])\n",
    "    print(str(smooth))\n",
    "    print(sentence)\n",
    "    print(s,ss, \"概率是\", P)\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function good_turing at 0x000002361A700BF8>\n",
      "[[('<BOS>', 'in'), 0.025641025641025644], [('in', 'a'), 0.5], [('a', 'hospital'), 0.16666666666666666], [('hospital', 'a'), 0.03333333333333333], [('a', 'student'), 0.16666666666666666], [('student', 'i'), 0.03333333333333333], [('i', 'be'), 0.5], [('be', '<EOS>'), 0.025641025641025644]]\n",
      "<BOS> in a hospital  a student i am <EOS> 概率是 5.07301077101647e-09\n",
      "<function Katz at 0x000002361A7001E0>\n",
      "[[('<BOS>', 'in'), 0.01190476190476191], [('in', 'a'), 0.5], [('a', 'hospital'), 0.16666666666666666], [('hospital', 'a'), 0.05357142857142859], [('a', 'student'), 0.16666666666666666], [('student', 'i'), 0.035714285714285726], [('i', 'be'), 0.5], [('be', '<EOS>'), 0.04761904761904764]]\n",
      "<BOS> in a hospital  a student i am <EOS> 概率是 7.532066114427642e-09\n",
      "<function absolute_discouting at 0x000002361A6A0E18>\n",
      "[[('<BOS>', 'in'), 0.02307692307692308], [('in', 'a'), 0.6], [('a', 'hospital'), 0.19999999999999998], [('hospital', 'a'), 0.02666666666666667], [('a', 'student'), 0.19999999999999998], [('student', 'i'), 0.02666666666666667], [('i', 'be'), 0.8], [('be', '<EOS>'), 0.02307692307692308]]\n",
      "<BOS> in a hospital  a student i am <EOS> 概率是 7.271005917159766e-09\n",
      "<function linear_discouting at 0x000002361A700E18>\n",
      "[[('<BOS>', 'in'), 0.038461538461538464], [('in', 'a'), 0.0], [('a', 'hospital'), 0.0], [('hospital', 'a'), 0.06666666666666667], [('a', 'student'), 0.0], [('student', 'i'), 0.06666666666666667], [('i', 'be'), 1.0], [('be', '<EOS>'), 0.038461538461538464]]\n",
      "<BOS> in a hospital  a student i am <EOS> 概率是 0.0\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "\n",
    "file = 'train_LM.txt'\n",
    "s = \"in a hospital  a student I am .\"\n",
    "fre = frequency(file,2)\n",
    "\n",
    "\n",
    "p3 = calP123(s, 2, fre,good_turing) # good_turing\n",
    "p = calP123(s, 2, fre, Katz)#Katz回退法，里面有递归\n",
    "p2 = calP123(s, 2, fre, absolute_discouting)#绝对减值\n",
    "p2 = calP123(s, 2, fre, linear_discouting)#线性减值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function unsmooth at 0x000002360F823488>\n",
      "[[('<BOS>', 'such'), 0.0], [('such', 'a'), 0.6666666666666666], [('a', 'deep'), 0.015873015873015872], [('deep', 'neural'), 0.07142857142857142], [('neural', 'network'), 0.7142857142857143], [('network', 'learn'), 0.0], [('learn', 'architecture'), 0.0], [('architecture', '<EOS>'), 0.0]]\n",
      "<BOS> such as deep neural networks learning architectures <EOS> 概率是 0.0\n",
      "<function jiayifa at 0x000002360F750D08>\n",
      "[[('<BOS>', 'such'), 0.0004363001745200698], [('such', 'a'), 0.003371868978805395], [('a', 'deep'), 0.0017730496453900709], [('deep', 'neural'), 0.0009610764055742432], [('neural', 'network'), 0.0028929604628736743], [('network', 'learn'), 0.0004816955684007707], [('learn', 'architecture'), 0.000481000481000481], [('architecture', '<EOS>'), 0.00048355899419729207]]\n",
      "<BOS> such as deep neural networks learning architectures <EOS> 概率是 8.125404951193938e-25\n",
      "<function good_turing at 0x000002361A700BF8>\n",
      "[[('<BOS>', 'such'), 0.00015469008436559217], [('such', 'a'), 0.4], [('a', 'deep'), 0.008130081300813009], [('deep', 'neural'), 0.01904761904761905], [('neural', 'network'), 0.4166666666666667], [('network', 'learn'), 0.00024248302618816683], [('learn', 'architecture'), 0.0003470776065528252], [('architecture', '<EOS>'), 0.00024177949709864604]]\n",
      "<BOS> such as deep neural networks learning architectures <EOS> 概率是 8.124080503498977e-20\n",
      "<function absolute_discouting at 0x000002361A6A0E18>\n",
      "[[('<BOS>', 'such'), 9.695322233548534e-05], [('such', 'a'), 0.6222222222222222], [('a', 'deep'), 0.013756613756613757], [('deep', 'neural'), 0.04285714285714286], [('neural', 'network'), 0.6571428571428571], [('network', 'learn'), 0.0001508783274059705], [('learn', 'architecture'), 0.00017816650469711697], [('architecture', '<EOS>'), 0.00019342359767891685]]\n",
      "<BOS> such as deep neural networks learning architectures <EOS> 概率是 1.2152473800944536e-19\n",
      "<function linear_discouting at 0x000002361A700E18>\n",
      "[[('<BOS>', 'such'), 0.00017669045191980972], [('such', 'a'), 0.4444444444444444], [('a', 'deep'), 0.004703115814226925], [('deep', 'neural'), 0.04591836734693877], [('neural', 'network'), 0.5102040816326531], [('network', 'learn'), 0.0003233107015842224], [('learn', 'architecture'), 0.00040492387431162946], [('architecture', '<EOS>'), 0.00048355899419729207]]\n",
      "<BOS> such as deep neural networks learning architectures <EOS> 概率是 5.477590134038397e-19\n",
      "<function Katz at 0x000002361A7001E0>\n",
      "[[('<BOS>', 'such'), 0.00038531774717558457], [('such', 'a'), 0.4], [('a', 'deep'), 0.008130081300813009], [('deep', 'neural'), 0.01904761904761905], [('neural', 'network'), 0.4166666666666667], [('network', 'learn'), 0.0008463817181549148], [('learn', 'architecture'), 0.00010075972835177555], [('architecture', '<EOS>'), 0.015869657215404655]]\n",
      "<BOS> such as deep neural networks learning architectures <EOS> 概率是 1.345935212441725e-17\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "\n",
    "file = 'train1.txt'\n",
    "fre = frequency(file,2)\n",
    "\n",
    "s = \" such as deep neural networks learning architectures\"\n",
    "\n",
    "p = calP123(s, 2,fre, unsmooth) #未平滑\n",
    "p = calP123(s, 2,fre, jiayifa) #加一法\n",
    "p = calP123(s, 2,fre, good_turing) # good_turing\n",
    "p = calP123(s, 2, fre, absolute_discouting) #绝对减值\n",
    "p = calP123(s, 2, fre, linear_discouting) #线性减值\n",
    "p = calP123(s, 2,fre, Katz)   #Katz回退法，里面有递归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function Katz at 0x000002361A6A0E18>\n",
      "[[('<BOS>', '<BOS>', 'such'), 4.376425738714748e-05], [('<BOS>', 'such', 'a'), 0.4038569706709463], [('such', 'a', 'deep'), 0.08333333333333333], [('a', 'deep', 'neural'), 0.16666666666666666], [('deep', 'neural', 'network'), 0.5], [('neural', 'network', 'learn'), 0.00046240307320193644], [('network', 'learn', 'architecture'), 0.012509374979259162], [('learn', 'architecture', '<EOS>'), 0.01493131594664462], [('architecture', '<EOS>', '<EOS>'), 0.5153814602132937]]\n",
      "<BOS> <BOS> such as deep neural networks learning architectures <EOS> <EOS> 概率是 5.4634610910075036e-15\n"
     ]
    }
   ],
   "source": [
    "file = 'train1.txt'\n",
    "fre = frequency(file,3)\n",
    "s = \" such as deep neural networks learning architectures\"\n",
    "p = calP123(s, 3,fre, Katz)#Katz回退法，里面有递归，3gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
