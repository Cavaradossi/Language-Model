{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('train.txt', 'r') as f:\n",
    "    train = []\n",
    "    for line in f: # loops over all the lines in the corpus\n",
    "        line = line.strip() # strips off \\n \\r from the ends \n",
    "        if line: # Take only non empty lines\n",
    "            line = re.sub(r'[^a-zA-Z0-9\\s]', '', line) \n",
    "            line = line.lower()\n",
    "            line = re.sub(' +',' ', line) # Removes consecutive spaces\n",
    "            # add more pre-processing steps\n",
    "            train.append(line)\n",
    "\n",
    "with open('valid.txt', 'r') as f:\n",
    "    valid = []\n",
    "    for line in f: # loops over all the lines in the corpus\n",
    "        line = line.strip() # strips off \\n \\r from the ends \n",
    "        if line: # Take only non empty lines\n",
    "            line = re.sub(r'[^a-zA-Z0-9\\s]', '', line) \n",
    "            line = line.lower()\n",
    "            line = re.sub(' +',' ', line) # Removes consecutive spaces\n",
    "            valid.append(line)\n",
    "\n",
    "with open('test.txt', 'r') as f:\n",
    "    test = []\n",
    "    for line in f: # loops over all the lines in the corpus\n",
    "        line = line.strip() # strips off \\n \\r from the ends \n",
    "        if line: # Take only non empty lines\n",
    "            line = re.sub(r'[^a-zA-Z0-9\\s]', '', line) \n",
    "            line = line.lower()\n",
    "            line = re.sub(' +',' ', line) # Removes consecutive spaces\n",
    "            test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import brown\n",
    "from nltk.lm import MLE\n",
    "from nltk import bigrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import word_tokenize, sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedLine = []\n",
    "for i in range(len(train)):\n",
    "    paddedLine.append(list(pad_both_ends(word_tokenize(train[i]), n=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " 'aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydroquebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim',\n",
       " 'snackfood',\n",
       " 'ssangyong',\n",
       " 'swapo',\n",
       " 'wachter',\n",
       " '</s>',\n",
       " '</s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddedLine[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydroquebec ipo kia memotec mlx nahb punts rake regatta rubens sim snackfood ssangyong swapo wachter',\n",
       " 'pierre unk n years old will join the board as a nonexecutive director nov n',\n",
       " 'mr unk is chairman of unk nv the dutch publishing group',\n",
       " 'rudolph unk n years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate',\n",
       " 'a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than n years ago researchers reported',\n",
       " 'the asbestos fiber unk is unusually unk once it enters the unk with even brief exposures to it causing symptoms that show up decades later researchers said',\n",
       " 'unk inc the unit of new yorkbased unk corp that makes kent cigarettes stopped using unk in its unk cigarette filters in n',\n",
       " 'although preliminary findings were reported more than a year ago the latest results appear in today s new england journal of medicine a forum likely to bring new attention to the problem',\n",
       " 'a unk unk said this is an old story',\n",
       " 'we re talking about years ago before anyone heard of asbestos having any questionable properties']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 9933 items>\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9933"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<UNK>', 'u', 'a', '<UNK>', '<UNK>', 'deep', 'learn', 'computer')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab.lookup([\"fuck\", \"u\", \"a\",\"asshole\",\"hello\",\"deep\",\"learn\",\"computer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 3517686 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['japanese', 'employees', 'submit', 'suggestions', 'to', 'the', 'proliferation', 'of', 'these', 'functions', 'are', 'currently', 'yielding', 'well', 'over', 'n', 'is', 'backed', 'by', 'freddie']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(20, random_seed=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity(test) = inf\n"
     ]
    }
   ],
   "source": [
    "print (\"perplexity(test) =\", model.perplexity(str(test).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity(test) = inf\n"
     ]
    }
   ],
   "source": [
    "print (\"perplexity(test) =\", model.perplexity(\"deep learn computer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity(test) = inf\n"
     ]
    }
   ],
   "source": [
    "print (\"perplexity(test) =\", model.perplexity(\"fuck u asshole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity(test) = 15468.81306887537\n"
     ]
    }
   ],
   "source": [
    "print (\"perplexity(test) =\", model.perplexity(\"deep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity(test) = 1522.049182885436\n"
     ]
    }
   ],
   "source": [
    "print (\"perplexity(test) =\", model.perplexity(\"learn\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
