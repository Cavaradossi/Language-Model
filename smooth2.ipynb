{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n写了课件里的几个平滑算法：加一法，good-turing,Katz回退法，绝对减值，线性减值\\n加一法，good-turing,绝对减值，线性减值适用2gram。3gram时容易出现历史信息即前两个词的序列不存在概率为零的情况\\nKatz回退法可以3gram，回退用递归写的，3gram时算的比较慢。\\n\\n对未登录词，事先随机选择一个频率为1-3的已经记录的词，用来替换未登录词\\n\\n训练集corpus.txt. 用open(file, encoding='gb18030', errors='ignore')忽略错误\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "写了课件里的几个平滑算法：加一法，good-turing,Katz回退法，绝对减值，线性减值\n",
    "加一法，good-turing,绝对减值，线性减值适用2gram。3gram时容易出现历史信息即前两个词的序列不存在概率为零的情况\n",
    "Katz回退法可以3gram，回退用递归写的，3gram时算的比较慢。\n",
    "\n",
    "对未登录词，事先随机选择一个频率为1-3的已经记录的词，用来替换未登录词\n",
    "\n",
    "训练集corpus.txt. 用open(file, encoding='gb18030', errors='ignore')忽略错误\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import random\n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#处理句子，加n-1个\"<BOS> \"和\" <EOS>\"\n",
    "def chuli(s, n): \n",
    "\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    s = s.strip() #去掉换行符\n",
    "    for i in range(n-1):  #加n-1个\"<BOS> \"和\" <EOS>\"\n",
    "        s = \"<BOS> \" + s + \" <EOS>\"\n",
    "    return s\n",
    "\n",
    "#将句子切成n元词组\n",
    "def ngram_generator(s, n): \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    token = [token for token in s.split(\" \") if token != \"\"]\n",
    "\n",
    "    # Stemming and Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged = pos_tag(token)\n",
    "    token = []\n",
    "    for word, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:  # not supply tag in case of None\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            token.append(lemma)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "            token.append(lemma)\n",
    "\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    return ngrams\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#读取文档，将n元词组存入字典\n",
    "def diction(file, n, k): #参数k表示为每个句子增加（k-1)个<BOS>,<EOS>\n",
    "    with open(file, encoding='gb18030', errors='ignore') as f: #忽略错误\n",
    "        diction = Counter()\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            line = chuli(line, k)\n",
    "            sentence = ngram_generator(line, n)\n",
    "            diction += Counter(sentence)\n",
    "    return diction\n",
    "\n",
    "#生成词组的频率数组fre，fre[i]储存i元词组的频率字典, 0<=i<=n\n",
    "def cal_frequency(file,n):\n",
    "    fre = []\n",
    "    for i in range(1,n+1):\n",
    "        word = diction(file, i, n)\n",
    "        fre.append(word)\n",
    "    num = sum(fre[0].values())\n",
    "    fre = [{(\"num\",):num}] + fre       \n",
    "    return fre\n",
    "\n",
    "#对一个词word,计算未平滑的概率\n",
    "def unsmooth(word, n, frequency):\n",
    "    count = frequency[n].get(word, 0)\n",
    "    pre_word = word[:-1] if len(word[:-1]) >= 1 else (\"num\",)\n",
    "    p = count / frequency[n-1][pre_word]\n",
    "    return p\n",
    "        \n",
    "#对一个词word,计算加一法平滑的概率\n",
    "def jiayifa(word, n, frequency):\n",
    "    count = frequency[n].get(word, 0)\n",
    "    N = len(frequency[1]) - (2 if n > 1 else 0)\n",
    "    pre_word = word[:-1] if len(word[:-1]) >= 1 else (\"num\",)        \n",
    "    p = (count + 1) / (frequency[n-1][pre_word] + N)\n",
    "    return p\n",
    "\n",
    "#对一个词word,计算good-turing平滑的概率\n",
    "def good_turing(word, n, frequency):\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency)       \n",
    "        \n",
    "    pre_word = word[:-1] if len(word[:-1]) >= 1 else (\"num\",)\n",
    "    num = frequency[n-1][pre_word]\n",
    "    R_n = {} #记录出现次数为r的词个数\n",
    "    for key, value in frequency[n].items():\n",
    "        if key[:-1] == word[:-1]:\n",
    "            R_n[value] = R_n.get(value, 0) + 1                     \n",
    "    R_n[0] = len(frequency[1]) - sum(R_n.values()) #出现次数为0的词个数\n",
    "    RtoRxing = {} #记录r对应的r*\n",
    "    r_max = max(R_n.keys())\n",
    "    r = 0\n",
    "    while(r<r_max):\n",
    "        if r in R_n:\n",
    "            rj = r + 1\n",
    "            while(rj not in R_n):\n",
    "                rj += 1\n",
    "            RtoRxing[r] = R_n[rj] * rj / R_n[r]\n",
    "            r = rj\n",
    "        else:\n",
    "            r += 1\n",
    "    RtoRxing[r_max] = r_max\n",
    "    N = 0\n",
    "    for key in R_n:\n",
    "        N += R_n[key] * RtoRxing[key] #计算平滑后词的总数N\n",
    "    rk = frequency[n].get(word, 0)\n",
    "    p = RtoRxing[rk] / N\n",
    "    return p\n",
    "\n",
    "#对一个词word,计算Katz平滑的概率， Katz里面含递归，算的慢。\n",
    "#Katz是回退法可以算3gram，可以处理3gram中前两个词排列未出现的情况。\n",
    "def Katz(word, n, frequency):\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency) \n",
    "    if word in frequency[n]:\n",
    "        return good_turing(word, n, frequency)\n",
    "    else:\n",
    "        beta = 0\n",
    "        Na = 0\n",
    "        wordin = []\n",
    "        for word2 in frequency[n]:\n",
    "            if word2[:-1] == word[:-1]:\n",
    "                wordin.append(word2[-1])\n",
    "                beta += good_turing(word2, n, frequency)\n",
    "        for word3 in frequency[1]:\n",
    "            if word3 not in wordin:\n",
    "                pre_word = word[1:-1] + word3\n",
    "                Na += Katz(pre_word, n - 1, frequency)\n",
    "                              \n",
    "        alpha = (1- beta) / Na\n",
    "        return alpha * Katz(word[1:], n - 1, frequency)\n",
    "\n",
    "#对一个词word,计算绝对减值法平滑的概率，b为自由参数0<b<=1\n",
    "def absolute_discouting(word, n, frequency, b=0.4): #b为自由参数0<b<=1\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency)       \n",
    "    \n",
    "    N = frequency[n-1][word[:-1]]\n",
    "    num = 0\n",
    "    for key in frequency[n].keys():\n",
    "        if key[:-1] == word[:-1]:\n",
    "            num += 1\n",
    "    if word in frequency[n]:\n",
    "        p = (frequency[n][word] - b) / N\n",
    "    else:\n",
    "        p = b * num / (len(frequency[1]) - num) / N        \n",
    "    return p\n",
    "\n",
    "#对一个词word,计算线性减值法平滑的概率\n",
    "def linear_discouting(word, n, frequency):\n",
    "    if n == 1:\n",
    "        return unsmooth(word, 1, frequency)       \n",
    "    \n",
    "    N = frequency[n-1][word[:-1]]\n",
    "    num = 0\n",
    "    n1 = 0\n",
    "    for key in frequency[n].keys():\n",
    "        if key[:-1] == word[:-1]:\n",
    "            num += 1\n",
    "            if frequency[n][key] == 1:\n",
    "                n1 += 1\n",
    "    alpha = n1 / N\n",
    "    if word in frequency[n]:\n",
    "        p = frequency[n][word] * (1 - alpha) / N\n",
    "    else:\n",
    "        p = alpha / (len(frequency[1]) - num)       \n",
    "    return p\n",
    "  \n",
    "    \n",
    "\n",
    "#事先随机选择一个频率为1-3的已经记录的词，用来替换未登录词\n",
    "def select_oov(frequency): \n",
    "    oov = ('$$$$$$$',)\n",
    "    for i in range(500):\n",
    "        t = random.sample(frequency[1].keys(), 1)[0]\n",
    "        if 1<= frequency[1][t] <= 3:\n",
    "            oov = t\n",
    "            break\n",
    "    if oov == ('$$$$$$$',):\n",
    "        oov = random.sample(frequency[1].keys(), 1)[0]\n",
    "    return oov[0]\n",
    "    \n",
    "#计算句子s的概率， 加了try,历史信息即前n-1个词的排列在smooth频率数组中不存在时，概率为0.1 ** 20          \n",
    "def calP(s, n,frequency, smooth, oov = \"\"):\n",
    "    if oov == \"\":\n",
    "        oov = select_oov(frequency)\n",
    "    s = chuli(s, n)\n",
    "    t = ngram_generator(s, n)\n",
    "    P = 1\n",
    "    sentence = []\n",
    "    for word in t:\n",
    "        \n",
    "        word_ti = []#将word词组替换为word_ti\n",
    "        for i in range(len(word)):\n",
    "            if (word[i],) not in frequency[1]: #如果word[i]为未登录词，替换成oov\n",
    "                t = oov\n",
    "            else:\n",
    "                t = word[i]\n",
    "            word_ti.append(t)\n",
    "        word_ti = tuple(word_ti)\n",
    "        \n",
    "        if(word != word_ti): #############中间结果，可去掉\n",
    "            print(word,\"计算时替换成\",word_ti) #############中间结果，可去掉\n",
    "            \n",
    "        try:\n",
    "            pk = smooth(word_ti, n, frequency)\n",
    "        except:\n",
    "            print(\"历史信息即前\", str(n-1) ,\"个词的排序\",word_ti[:-1],\"未在训练集里出现过\")\n",
    "            pk = 0.1 ** 20\n",
    "        P *= pk\n",
    "        sentence.append([word,pk])\n",
    "    \n",
    "    pinghua = {unsmooth:\"未平滑\",jiayifa:\"加一法平滑\",good_turing:\"古德图灵平滑\",\n",
    "        absolute_discouting:\"绝对减值平滑\",linear_discouting:\"线性减值平滑\",Katz:\"Kazt平滑\"}\n",
    "    \n",
    "    print(sentence)   #############中间结果，可去掉\n",
    "    print(s,pinghua[smooth], \"概率是\", P) #############中间结果，可去掉\n",
    "    return P \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "\n",
    "file = 'corpus.txt'\n",
    "fre = cal_frequency(file,2) #2表示2gram\n",
    "oov = select_oov(fre) #事先随机选择一个频率为1-3的已经记录的词，用来替换未登录词\n",
    "s = \" such as deep ssssssssss neural networks learning architectures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep', 'sss') 计算时替换成 ('deep', 'ukrainian')\n",
      "('sss', 'neural') 计算时替换成 ('ukrainian', 'neural')\n",
      "[[('<BOS>', 'such'), 0.0021946564885496184], [('such', 'a'), 0.6013986013986014], [('a', 'deep'), 0.001204291657543245], [('deep', 'sss'), 0.0], [('sss', 'neural'), 0.0], [('neural', 'network'), 0.09722222222222222], [('network', 'learn'), 0.0], [('learn', 'architecture'), 0.0], [('architecture', '<EOS>'), 0.06666666666666667]]\n",
      "<BOS> such as deep ssssssssss neural networks learning architectures <EOS> 未平滑 概率是 0.0\n"
     ]
    }
   ],
   "source": [
    "p = calP(s, 2,fre, unsmooth, oov) #未平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep', 'sss') 计算时替换成 ('deep', 'ukrainian')\n",
      "('sss', 'neural') 计算时替换成 ('ukrainian', 'neural')\n",
      "[[('<BOS>', 'such'), 0.0007066721630057122], [('such', 'a'), 0.010831834720421563], [('a', 'deep'), 0.0003679175864606328], [('deep', 'sss'), 4.251519918370818e-05], [('sss', 'neural'), 4.258399693395222e-05], [('neural', 'network'), 0.00033964507090090854], [('network', 'learn'), 4.245923913043478e-05], [('learn', 'architecture'), 4.2468254979402894e-05], [('architecture', '<EOS>'), 0.0001275944198707043]]\n",
      "<BOS> such as deep ssssssssss neural networks learning architectures <EOS> 加一法平滑 概率是 3.984325562425389e-34\n"
     ]
    }
   ],
   "source": [
    "p = calP(s, 2,fre, jiayifa, oov) #加一法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep', 'sss') 计算时替换成 ('deep', 'ukrainian')\n",
      "('sss', 'neural') 计算时替换成 ('ukrainian', 'neural')\n",
      "[[('<BOS>', 'such'), 0.001017915309446254], [('such', 'a'), 0.37554585152838427], [('a', 'deep'), 0.00117096018735363], [('deep', 'sss'), 2.2243312177471973e-05], [('sss', 'neural'), 2.129199846697611e-05], [('neural', 'network'), 0.1956521739130435], [('network', 'learn'), 1.45757485456642e-05], [('learn', 'architecture'), 1.2026747486409776e-05], [('architecture', '<EOS>'), 0.08108108108108109]]\n",
      "<BOS> such as deep ssssssssss neural networks learning architectures <EOS> 古德图灵平滑 概率是 5.895435553325106e-28\n"
     ]
    }
   ],
   "source": [
    "p = calP(s, 2,fre, good_turing, oov) # good_turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep', 'sss') 计算时替换成 ('deep', 'ukrainian')\n",
      "('sss', 'neural') 计算时替换成 ('ukrainian', 'neural')\n",
      "[[('<BOS>', 'such'), 0.0021564885496183207], [('such', 'a'), 0.6004662004662005], [('a', 'deep'), 0.0011604992336325815], [('deep', 'sss'), 1.2243327386574318e-05], [('sss', 'neural'), 1.703359877358089e-05], [('neural', 'network'), 0.09166666666666666], [('network', 'learn'), 8.77321245796169e-06], [('learn', 'architecture'), 8.396856426875188e-06], [('architecture', '<EOS>'), 0.05333333333333334]]\n",
      "<BOS> such as deep ssssssssss neural networks learning architectures <EOS> 绝对减值平滑 概率是 1.1286830370497503e-28\n"
     ]
    }
   ],
   "source": [
    "p = calP(s, 2, fre, absolute_discouting, oov) #绝对减值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep', 'sss') 计算时替换成 ('deep', 'ukrainian')\n",
      "('sss', 'neural') 计算时替换成 ('ukrainian', 'neural')\n",
      "[[('<BOS>', 'such'), 0.0019094349105529982], [('such', 'a'), 0.4836422318939801], [('a', 'deep'), 0.0009485082312640798], [('deep', 'sss'), 2.6235701542659253e-05], [('sss', 'neural'), 4.258399693395222e-05], [('neural', 'network'), 0.08371913580246915], [('network', 'learn'), 1.644977335867817e-05], [('learn', 'architecture'), 1.4432096983691731e-05], [('architecture', '<EOS>'), 0.04]]\n",
      "<BOS> such as deep ssssssssss neural networks learning architectures <EOS> 线性减值平滑 概率是 7.7800675605487655e-28\n"
     ]
    }
   ],
   "source": [
    "p = calP(s, 2, fre, linear_discouting, oov) #线性减值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deep', 'sss') 计算时替换成 ('deep', 'ukrainian')\n",
      "('sss', 'neural') 计算时替换成 ('ukrainian', 'neural')\n",
      "[[('<BOS>', 'such'), 0.001017915309446254], [('such', 'a'), 0.37554585152838427], [('a', 'deep'), 0.00117096018735363], [('deep', 'sss'), 1.516396990205584e-06], [('sss', 'neural'), 0.00010463139232418534], [('neural', 'network'), 0.1956521739130435], [('network', 'learn'), 6.45668402000512e-05], [('learn', 'architecture'), 2.459284862320591e-05], [('architecture', '<EOS>'), 0.08108108108108109]]\n",
      "<BOS> such as deep ssssssssss neural networks learning architectures <EOS> Kazt平滑 概率是 1.789016066731929e-27\n"
     ]
    }
   ],
   "source": [
    "p = calP(s, 2,fre, Katz, oov)   #Katz回退法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'corpus.txt'\n",
    "fre = cal_frequency(file,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'deep', 'sss') 计算时替换成 ('a', 'deep', 'ukrainian')\n"
     ]
    }
   ],
   "source": [
    "p = calP(s, 3,fre, Katz, oov)#Katz回退法，里面有递归，3gram算地慢，笔记本跑一个小时没跑出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
